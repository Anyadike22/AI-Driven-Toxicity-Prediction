# -*- coding: utf-8 -*-
"""AI-Driven Toxicity Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHO2DreJbaI6z2qt2C-Vl3ghhZo24V9h
"""

# Install required packages
!pip install deepchem rdkit-pypi shap xgboost

#Import libraries
import numpy as np
import pandas as pd
import deepchem as dc
from rdkit import Chem
from rdkit.Chem import AllChem
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, classification_report, roc_curve
import shap
import matplotlib.pyplot as plt

#Load Tox21 dataset from DeepChem
tox21_tasks, tox21_datasets, _ = dc.molnet.load_tox21()
train_dataset, valid_dataset, test_dataset = tox21_datasets

#Convert DeepChem dataset to pandas DataFrame
def extract_features_labels(dataset):
    features = dataset.X
    labels = dataset.y
    smiles = dataset.ids  # IDs are SMILES strings in DeepChem
    return pd.DataFrame({'smiles': smiles, 'features': list(features), 'label': list(labels[:, 0])})

train_df = extract_features_labels(train_dataset)
test_df = extract_features_labels(test_dataset)

# Generate molecular fingerprints using RDKit
def smiles_to_fingerprint(smiles, radius=2, n_bits=2048):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(n_bits)  # Handle invalid SMILES
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
    return np.array(fp)

train_df['fingerprint'] = train_df['smiles'].apply(smiles_to_fingerprint)
test_df['fingerprint'] = test_df['smiles'].apply(smiles_to_fingerprint)

# Convert fingerprint lists to numpy arrays
X_train = np.stack(train_df['fingerprint'].values)
y_train = train_df['label'].values
X_test = np.stack(test_df['fingerprint'].values)
y_test = test_df['label'].values

# Handle class imbalance
print(f"Class distribution:\nTrain: {pd.Series(y_train).value_counts(normalize=True)}\nTest: {pd.Series(y_test).value_counts(normalize=True)}")

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)

# Train XGBoost model
xgb_model = XGBClassifier(n_estimators=100, scale_pos_weight=sum(y_train == 0) / sum(y_train == 1),
                          use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Model evaluation function
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

from sklearn.metrics import roc_curve

def evaluate_model(model, X_test, y_test):
    """Evaluate classification model performance."""

    # Generate predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1

    # Print classification report
    print(classification_report(y_test, y_pred))

    # Compute and print ROC AUC score
    roc_auc = roc_auc_score(y_test, y_proba)
    print(f"ROC AUC: {roc_auc:.3f}")

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.3f})", color="blue")
    plt.plot([0, 1], [0, 1], linestyle="--", color="red")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.show()

print("Random Forest Performance:")
evaluate_model(rf_model, X_test, y_test)

print("\nXGBoost Performance:")
evaluate_model(xgb_model, X_test, y_test)

# Feature importance analysis using SHAP
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

plt.title('Feature Importance Summary')
shap.summary_plot(shap_values, X_test, plot_type="bar", max_display=20)

#End